{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_reader(datafile):\n",
    "    \n",
    "    return pd.read_csv(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=dataset_reader('housing.csv')\n",
    "df2=dataset_reader('concreteData.csv')\n",
    "df3=dataset_reader('yachtData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \n",
    "    def __init__(self,X,y,learningRate,tolerance,lamda,maxIteration=50000,error='rmse',gd=False,sgd=False):\n",
    "        \n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.learningRate=learningRate\n",
    "        self.tolerance=tolerance\n",
    "        self.maxIteration=maxIteration\n",
    "        self.error=error\n",
    "        self.gd=gd\n",
    "        self.lamda=lamda\n",
    "        self.sgd=sgd\n",
    "        \n",
    "    def splitTraintest(self):\n",
    "        \n",
    "        X_train,X_test,y_train,y_test=train_test_split(self.X,self.y,test_size=0.2,random_state=0)\n",
    "        \n",
    "        return  X_train,X_test,y_train,y_test\n",
    "    \n",
    "    def add_x0(self,X):\n",
    "        \n",
    "        return np.column_stack([np.ones([X.shape[0],1]),X])\n",
    "    \n",
    "    def normalize(self,X):\n",
    "        \n",
    "        mean=np.mean(X)\n",
    "        std=np.std(X)\n",
    "        X_norm=(X-mean)/std\n",
    "        X_norm=self.add_x0(X_norm)\n",
    "        \n",
    "        return X_norm,mean,std\n",
    "    \n",
    "    def normalizeTestData(self,X_test,train_mean,train_std):\n",
    "        \n",
    "        X_norm=(X_test-train_mean)/train_std\n",
    "        X_norm=self.add_x0(X_norm)\n",
    "        \n",
    "        return X_norm\n",
    "    \n",
    "    def rank(self,X,eps=1e-12):\n",
    "        \n",
    "        u,s,vh=np.linalg.svd(X)\n",
    "        \n",
    "        return len([x for x in s if abs(s) >eps])\n",
    "    \n",
    "    def checkMatrix(self,X):\n",
    "        \n",
    "        X_rank=np.linalg.matrix_rank(X)\n",
    "        \n",
    "        if X_rank==min(X.shape[0],X.shape[1]):\n",
    "            \n",
    "            self.full_rank=True\n",
    "            print(\"Data is Full Rank\")\n",
    "            \n",
    "        else:\n",
    "            self.full_rank=False\n",
    "            print(\"Data is not Full Rank\")\n",
    "            \n",
    "    def checkInvertibility(self,X):\n",
    "        if X.shape[0]<X.shape[1]:\n",
    "                self.lowRank=True\n",
    "                print(\"Matrix is Low Rank\")\n",
    "                \n",
    "        else:\n",
    "                self.lowRank=False\n",
    "                print(\"Matrix is not Low Rank\")\n",
    "                \n",
    "    def closedFormSolution(self,X,y):\n",
    "            \n",
    "            sizeofi=X.T.dot(X)\n",
    "            identity=np.eye(sizeofi.shape[0],sizeofi.shape[1])\n",
    "            self.theta=np.linalg.inv(X.T.dot(X)+self.lamda*identity).dot(X.T).dot(y)\n",
    "            return self.theta\n",
    "    \n",
    "    \n",
    "    def gradientDescent(self,X,y):\n",
    "        \n",
    "        self.errors=[]\n",
    "        last_error=float('inf')\n",
    "        \n",
    "        for t in tqdm(range(self.maxIteration)):\n",
    "            self.theta=self.theta-self.learningRate * (self.cost_derivative(X,y)+(self.lamda*self.theta))\n",
    "            \n",
    "            if self.error=='rmse':\n",
    "                current_error=self.rmse(X,y)\n",
    "            else:\n",
    "                current_error=self.sse(X,y)\n",
    "                \n",
    "            error_difference=last_error-current_error\n",
    "            last_error=current_error\n",
    "        \n",
    "            self.errors.append(current_error)\n",
    "           \n",
    "            if error_difference<self.tolerance:\n",
    "                print(\"The model stopped learning - Converged\")\n",
    "                break\n",
    "                \n",
    "        return  self.plot_function(self.errors)\n",
    "    \n",
    "    def stochasticgradientDescent(self,X,y):\n",
    "        \n",
    "        self.errors=[]\n",
    "        last_error=float('inf')\n",
    "        \n",
    "        for t in tqdm(range(self.maxIteration)):\n",
    "            index=[random.randint(0,X.shape[0]-1) for i in np.arange(1,100,1)]\n",
    "            X=X[index]\n",
    "            y=y[index]\n",
    "            \n",
    "            b=0.0000001\n",
    "            c=0.001\n",
    "        \n",
    "            alphat=b/(t+c)\n",
    "           \n",
    "            self.theta=self.theta-alphat * (self.cost_derivative(X,y)+(self.lamda*self.theta))\n",
    "            \n",
    "            if self.error=='rmse':\n",
    "                current_error=self.rmse(X,y)\n",
    "            else:\n",
    "                current_error=self.sse(X,y)\n",
    "                \n",
    "            error_difference=last_error-current_error\n",
    "            last_error=current_error\n",
    "        \n",
    "            self.errors.append(current_error)\n",
    "           \n",
    "            if abs(error_difference)<self.tolerance:\n",
    "                print(\"The model stopped learning - Converged\")\n",
    "                break\n",
    "        return  self.plot_function(self.errors)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \n",
    "        return X.dot(self.theta)\n",
    "    \n",
    "    def sse(self,X,y):\n",
    "        \n",
    "        y_hat=self.predict(X)\n",
    "        return((y_hat-y)**2).sum()\n",
    "    \n",
    "    def rmse(self,X,y):\n",
    "        \n",
    "        return np.sqrt(self.sse(X,y)/X.shape[0])\n",
    "    \n",
    "    def cost_derivative(self,X,y):\n",
    "        y_hat=self.predict(X)\n",
    "        grad=(y_hat-y).dot(X)\n",
    "        return grad\n",
    "    \n",
    "    def plot_function(self,datatoplot):\n",
    "        \n",
    "        return plt.plot(datatoplot)\n",
    "    \n",
    "    def run_model(self):\n",
    "        \n",
    "        self.X_train,self.X_test,self.y_train,self.y_test=self.splitTraintest()\n",
    "        \n",
    "        self.X_train,self.mean,self.std=self.normalize(self.X_train)\n",
    "        self.X_test=self.normalizeTestData(self.X_test,self.mean,self.std)\n",
    "        \n",
    "        self.checkMatrix(self.X_train)\n",
    "        self.checkInvertibility(self.X_train)\n",
    "        \n",
    "        if self.full_rank and not self.lowRank and self.X_train.shape[0]<=10000 and not self.gd and not self.sgd:\n",
    "            self.theta=self.closedFormSolution(self.X_train,self.y_train)\n",
    "            \n",
    "        elif self.gd:\n",
    "            self.theta=np.ones(self.X_train.shape[1],dtype=np.float64)*0\n",
    "            self.gradientDescent(self.X_train,self.y_train)\n",
    "            \n",
    "        elif self.sgd:\n",
    "            self.theta=np.ones(self.X_train.shape[1],dtype=np.float64)*0\n",
    "            self.stochasticgradientDescent(self.X_train,self.y_train)\n",
    "            \n",
    "        print(self.theta)\n",
    "        \n",
    "      \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression=LinearRegression(df1.values[:,0:-1],df1.values[:,-1],learningRate=0.00001,tolerance=0.0000001,lamda=1,error='rmse',sgd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 280/50000 [00:00<00:18, 2723.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is Full Rank\n",
      "Matrix is not Low Rank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 8911/50000 [00:02<00:13, 3159.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model stopped learning - Converged\n",
      "[ 0.22406179 -0.10524228 -0.08686168 -0.09293171 -0.10809833 -0.10740399\n",
      " -0.09824598 -0.01247339 -0.10207274 -0.09648295  0.46065139 -0.08066379\n",
      "  0.46209884 -0.09235245]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "regression.run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45948.10720306067"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression.sse(regression.X_test,regression.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.32912924818426"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression.rmse(regression.X_test,regression.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.33167856, 2.35577231, 3.50446614, 2.36266749, 3.56175232,\n",
       "       2.42508791, 2.43994787, 3.54313666, 2.56870514, 2.5051025 ,\n",
       "       2.31581625, 2.42643776, 2.4333001 , 2.43428256, 2.41859033,\n",
       "       2.50358896, 3.42007232, 2.24498867, 1.76271149, 2.35765057,\n",
       "       2.34561133, 3.54363276, 3.56612595, 2.67333973, 2.22055378,\n",
       "       3.31757125, 2.44410016, 3.4448805 , 2.36253924, 2.43316701,\n",
       "       3.55436789, 2.83043629, 2.02466496, 2.34519216, 2.35083562,\n",
       "       3.42632756, 2.43802481, 2.46171505, 2.15665622, 2.70536699,\n",
       "       2.39214278, 2.19924959, 3.29479508, 2.4064981 , 2.70910086,\n",
       "       2.24008146, 2.31202632, 2.73904958, 2.30339768, 2.68530831,\n",
       "       2.71612726, 2.59330127, 2.43820287, 2.0252862 , 2.82564273,\n",
       "       2.45338178, 2.41885641, 2.41158979, 3.31050519, 2.0636821 ,\n",
       "       2.73950923, 2.16325257, 3.50463033, 2.55317728, 2.1594953 ,\n",
       "       2.49191662, 2.45560004, 2.86206289, 2.37988053, 2.66311823,\n",
       "       2.7387015 , 2.50718875, 2.5998723 , 2.18347217, 2.41244013,\n",
       "       2.38835457, 3.46505983, 2.69343474, 2.71586798, 2.3408889 ,\n",
       "       2.48947462, 2.37582448, 2.43046656, 3.54145514, 2.36926127,\n",
       "       2.76060856, 2.33923351, 2.61599952, 2.29626518, 2.14724467,\n",
       "       3.19251155, 3.3560505 , 2.16771454, 3.42974496, 2.09248017,\n",
       "       2.36906318, 2.22783804, 3.31091526, 2.2977031 , 2.71151389,\n",
       "       2.687702  ])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression.predict(regression.X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
